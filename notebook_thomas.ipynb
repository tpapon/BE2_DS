{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2653bf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For embeddings and similarity computation\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"Required libraries imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing library: {e}\")\n",
    "    print(\"Please install with: pip install sentence-transformers scikit-learn networkx\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15048de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import find, csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.linalg import norm\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# des options permettent de limiter (ou non) le nombre de lignes/colonnes affichées\n",
    "# par exemple :\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# cette fonction permet d'afficher une \"jolie\" représentation du vecteur v\n",
    "# ARGS :\n",
    "#   v : le vecteur à afficher (par ex. une ligne de la matrice X)\n",
    "#   features : le vocabulaire\n",
    "#   top_n : le nombre de mots maximum à afficher\n",
    "def print_feats(v, features, top_n = 30):\n",
    "    _, ids, values = find(v)\n",
    "    feats = [(ids[i], values[i], features[ids[i]]) for i in range(len(list(ids)))]\n",
    "    top_feats = sorted(feats, key=lambda x: x[1], reverse=True)[0:top_n]\n",
    "    return pd.DataFrame({\"word\" : [t[2] for t in top_feats], \"value\": [t[1] for t in top_feats]})   \n",
    "\n",
    "# fonction qui permet d'afficher plusieurs tables pandas côte à côte (c'est cadeau)\n",
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"&emsp;\"\n",
    "        #output += \"\\xa0\\xa0\\xa0\"\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecf5c0",
   "metadata": {},
   "source": [
    "# 1. Chargement et prise en main des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a74280e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 25657 documents in corpus\n",
      "Loaded 1000 queries\n",
      "Loaded relevance for 700 queries (dataset)\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(file_path: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load corpus data from a JSONL file.\n",
    "    Returns a dictionary mapping document IDs to their associated metadata.\n",
    "    \"\"\"\n",
    "    corpus = {}\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # skip empty lines\n",
    "                doc = json.loads(line)\n",
    "                doc_id = doc.get(\"_id\")\n",
    "                if doc_id is not None:\n",
    "                    corpus[doc_id] = doc\n",
    "                    \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def load_queries(file_path: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Load query data from a JSONL file.\n",
    "    Returns a dictionary mapping query IDs to query metadata.\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                q = json.loads(line)\n",
    "                q_id = q.get(\"_id\")\n",
    "                if q_id is not None:\n",
    "                    queries[q_id] = q\n",
    "                    \n",
    "    return queries\n",
    "\n",
    "\n",
    "def load_qrels(file_path: str) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Load relevance judgments from a TSV file.\n",
    "    Returns a dictionary:\n",
    "        { query_id : { candidate_id : relevance_score } }\n",
    "    \"\"\"\n",
    "    qrels = {}\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        next(f) #On saute l'en-tête\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 3:\n",
    "                continue  \n",
    "            \n",
    "            q_id, c_id, score = parts\n",
    "            score = int(score)\n",
    "\n",
    "            if q_id not in qrels:\n",
    "                qrels[q_id] = {}\n",
    "            qrels[q_id][c_id] = score\n",
    "    \n",
    "    return qrels\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "corpus = load_corpus('dataset_BE2/corpus.jsonl')\n",
    "queries = load_queries('dataset_BE2/queries.jsonl')\n",
    "qrels_valid = load_qrels('dataset_BE2/valid.tsv')\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(corpus)} documents in corpus\")\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "print(f\"Loaded relevance for {len(qrels_valid)} queries (dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fea64",
   "metadata": {},
   "source": [
    "# 2 - Exploration des données et premier encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6724756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du corpus : 25657 documents\n",
      "Nombre de requêtes : 1000\n",
      "Nombre total de paires requête/document : 20950\n",
      "Proportion d'articles pertinents par requète : 16%\n"
     ]
    }
   ],
   "source": [
    "nb_docs = len(corpus)\n",
    "nb_queries = len(queries)\n",
    "nb_pairs = sum(len(docs) for docs in qrels_valid.values()) #on prend chaque dictionnaire de documents pour chaque requête et on somme\n",
    "proportions = []\n",
    "for qid, docs in qrels_valid.items(): ##[..].items contient une ligne par requête avec id de chaque document et score\n",
    "    total_docs = len(docs)\n",
    "    if total_docs == 0:\n",
    "        continue  \n",
    "    num_positifs = sum(1 for score in docs.values() if score == 1)\n",
    "    proportion = num_positifs / total_docs\n",
    "    proportions.append(proportion)\n",
    "\n",
    "moyenne_proportion = sum(proportions) / len(proportions)\n",
    "\n",
    "print(f\"Taille du corpus : {nb_docs} documents\")\n",
    "print(f\"Nombre de requêtes : {nb_queries}\")\n",
    "print(f\"Nombre total de paires requête/document : {nb_pairs}\")\n",
    "print(f\"Proportion d'articles pertinents par requète : {round(moyenne_proportion*100)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed8d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requête\n",
      "ID : 78495383450e02c5fe817e408726134b3084905d\n",
      "Titre : A Direct Search Method to solve Economic Dispatch Problem with Valve-Point Effect\n",
      "\n",
      "Candidats positifs\n",
      "- A Hybrid EP and SQP for Dynamic Economic Dispatch with Nonsmooth Fuel Cost Function\n",
      "- A modified particle swarm optimizer\n",
      "- Identification and control of dynamic systems using recurrent fuzzy neural networks\n",
      "\n",
      "Candidats négatifs\n",
      "- A data mining approach for location prediction in mobile environments\n",
      "- Treatment-Response Models for Counterfactual Reasoning with Continuous-time, Continuous-valued Interventions\n",
      "- Dissecting and Reassembling Color Correction Algorithms for Image Stitching\n"
     ]
    }
   ],
   "source": [
    "def show_example_query(corpus, queries, qrels, q_id=None):\n",
    "\n",
    "    if q_id is None:\n",
    "        q_id = next(iter(queries))  # prend la première requete\n",
    "\n",
    "    query = queries[q_id]\n",
    "\n",
    "    print(\"Requête\")\n",
    "    print(\"ID :\", q_id)\n",
    "    print(\"Titre :\", query.get(\"text\"))\n",
    "    print()\n",
    "\n",
    "    # On sépare les positifs des négatifs\n",
    "    candidates = qrels[q_id]\n",
    "    positives = [c for c, s in candidates.items() if s == 1]\n",
    "    negatives = [c for c, s in candidates.items() if s == 0]\n",
    "\n",
    "    # On affiche des candidats\n",
    "    print(\"Candidats positifs\")\n",
    "    for c_id in positives[:3]:\n",
    "        print(\"-\", corpus[c_id][\"title\"])\n",
    "    print()\n",
    "\n",
    "    print(\"Candidats négatifs\")\n",
    "    for c_id in negatives[:3]:\n",
    "        print(\"-\", corpus[c_id][\"title\"])\n",
    "\n",
    "show_example_query(corpus, queries, qrels_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
